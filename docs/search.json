[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kinnary H. Shah",
    "section": "",
    "text": "Thanks for taking a look at my website!\nMy name is Kinnary (kin-uh-ree) Shah and I am working on my PhD at Johns Hopkins Bloomberg School of Public Health in the Department of Biostatistics. I received my Bachelor’s degree in Biomathematics & Public Health from Rutgers University - New Brunswick, where I completed research projects in computational genetics and public policy. Outside of academics, I enjoy experimenting in the kitchen and hand building slab pottery."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Research",
    "section": "",
    "text": "In collaboration with Dr. Stephanie Hicks and researchers at the Lieber Institute for Brain Development, I am currently working on spatial transcriptomics methods development and data analysis projects.\nI presented one of my projects at the BioConductor conference in August 2023. Feel free to watch it here (0:40-10:22)! If the embedded video isn’t working, you can use this link."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "I have served as a teaching assistant for the following courses:\n\nLead TA Statistical Methods in Public Health (2023-2024, 2024-2025)\nMultilevel Models (June 2023, June 2024)\nStatistical Methods in Public Health I-IV (2022-2023)"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "simulations part 1\n\n\n\n\n\n\n\nFDR\n\n\ngenomics\n\n\n\n\nunderstanding FDR in a basic differential expression analysis\n\n\n\n\n\n\nMay 6, 2024\n\n\nKinnary H. Shah\n\n\n\n\n\n\n  \n\n\n\n\npreliminary oral exam\n\n\n\n\n\n\n\noral exam\n\n\nstudying\n\n\n\n\nhow I prepared for (and passed!) my preliminary oral exam\n\n\n\n\n\n\nDec 15, 2023\n\n\nKinnary H. Shah\n\n\n\n\n\n\n  \n\n\n\n\nrandom seeds\n\n\n\n\n\n\n\nR\n\n\nrandom seed\n\n\n\n\nwhat I learned about random seeds while setting up a simulation\n\n\n\n\n\n\nAug 9, 2023\n\n\nKinnary H. Shah\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-08-09-random-seeds/index.html",
    "href": "posts/2023-08-09-random-seeds/index.html",
    "title": "random seeds",
    "section": "",
    "text": "TLDR: I explain why researchers should not use the same random seed to generate variables that should not be correlated.\nI have been working on a simulation for a spatial transcriptomics project. During the process of debugging the simulation, I came across an issue with setting seeds that I wanted to explore further. Setting a seed initializes a value that is used by a pseudorandom number generator.\nComparing these two lines of code shows us how a seed functions:\n\nset.seed(10); runif(2); runif(2)\n\n[1] 0.5074782 0.3067685\n\n\n[1] 0.4269077 0.6931021\n\nset.seed(10); runif(4)\n\n[1] 0.5074782 0.3067685 0.4269077 0.6931021\n\n\nFor this reason, we want to refrain from using the same random seed to generate variables that should not be correlated. For example,\n\nset.seed(1)\nalpha &lt;- runif(50, 1, 2)\n\nset.seed(1)\nbeta &lt;- runif(25, 0.5, 4)\nbeta &lt;- append(beta, runif(25, 4, 8))\n\nplot(alpha, beta)\n\n\n\ncor(alpha[1:25], beta[1:25])\n\n[1] 1\n\ncor(alpha[26:50], beta[26:50])\n\n[1] 1\n\n\nIt is very clear when I write it in simple code like this, but I was not aware of this concern before. The same issue occurs when selecting values from any distribution.\n\nset.seed(100)\nvals1 &lt;- rnorm(100, mean = 3, sd = 1)\n\nset.seed(100)\nvals2 &lt;- rnorm(100, mean = 0.5, sd = 10)\n\nplot(vals1, vals2)\n\n\n\ncor(vals1, vals2)\n\n[1] 1\n\n\nTo make sure this result is not by chance, I replicated it with 50 different seeds and summarized the correlation.\n\ncorrelation_list &lt;- c()\n\nfor(i in c(1:50)){\n  set.seed(i)\n  vals1 &lt;- rnorm(100, mean = i, sd = 1)\n\n  set.seed(i)\n  vals2 &lt;- rnorm(100, mean = 0.5, sd = i)\n\n  correlation_list &lt;- append(correlation_list, cor(vals1, vals2))\n}\n\ncorrelation_list\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[39] 1 1 1 1 1 1 1 1 1 1 1 1\n\n\n\n\n\n\n\n\nBest Practices\n\n\n\nFrom now on, I will be implementing a better practice of setting the seed just once at the beginning (Morris and Crowther 2019).\n\n\nI was under the assumption that using the same seed to simulate, say, normal and uniform data would produce correlated data as well. I found that this is actually not the case and that this correlation does not occur when selecting values from different distributions.\n\nset.seed(9)\nnormal &lt;- rnorm(100)\n\nset.seed(9)\nuniform &lt;- runif(100)\n\nplot(normal, uniform)\n\n\n\ncor(normal, uniform)\n\n[1] 0.06719044\n\n\nThis finding is counterintuitive to me as I thought that generating values from the normal distribution was just transformation of randomly generated uniform values to the normal distribution. I’m reading about the Mersenne Twister, a modern random number generator that is used in R and other languages (“Mersenne Twister” 2023), in hopes of understanding this. I would love to hear if anyone knows of an explanation for this.\nBig thanks to Boyi Guo for his insights on this post!\n\n\n\n\n\n\nCoffee\n\n\n\nIf you found this blog post helpful and would like to support my work, feel free to buy me a coffee.\n\n\n\n\n\n\nReferences\n\n“Mersenne Twister.” 2023. https://en.m.wikipedia.org/wiki/Mersenne_Twister.\n\n\nMorris, White, T., and M. Crowther. 2019. “Using Simulation Studies to Evaluate Statistical Methods.” https://onlinelibrary.wiley.com/doi/10.1002/sim.8086."
  },
  {
    "objectID": "posts/2023-12-15-oral-exam/index.html",
    "href": "posts/2023-12-15-oral-exam/index.html",
    "title": "preliminary oral exam",
    "section": "",
    "text": "The main challenge in the third year of the PhD program in Biostatistics at Johns Hopkins Bloomberg School of Public Health is the preliminary oral exam. The oral exam is interesting in that it is not a defense of a specific research proposal, rather it is intended to evaluate your capacity to be a strong researcher and your foundation in biostatistics. I was pretty nervous about this exam and wanted to write this post to share some tips for future students, heavily inspired by Albert Kuo’s (“My Preliminary Oral Exam” 2020) very helpful post.\nOne of the biggest challenges was actually scheduling the exam, which our very knowledgeable academic administrator helped me out with. There are several guidelines to follow when creating your committee and alternates to ensure a very specific mix of departments and faculty ranks/appointments. After checking with each person individually to make sure they were willing to be on my committee, we sent out emails to find a two hour window in which everyone was available. For context, I started searching for a date on August 18, 2023 and finalized it on September 11, 2023. This process was so long because faculty members have incredibly busy schedules and coordinating their schedules is difficult. I ended up asking a couple members to switch around their schedules to find a time that worked for everyone, and they kindly accommodated my request. My tip for scheduling is to start as early as possible, at least two months before your intended date.\nOnce I had the exam scheduled, the next steps were writing up a proposal and preparing for possible questions. The proposal is due one month before your oral exam, so I spent about one hour per weekday writing to make sure I could complete it in time. I usually chose the 8am-9am time slot to focus on writing before I started getting distracted by Slack and email notifications. My proposal ended up being about 50 pages in LaTeX with an introduction, two chapters, and a conclusion. The chapters were similar to in-progress research manuscripts for each of the two projects I was working on. About two weeks before my proposal was due, I asked my advisor to read it over and give me some comments. After addressing her comments, I had a completed proposal. I also had to create a 15 minute slides-based presentation for my oral exam based on my proposal. Since my proposal was already written, I found this pretty easy to put together since I already had the language and figures ready.\n\n\n\n\n\n\nOn Writing\n\n\n\nSome days, “writing” was really staring at my computer for 30 minutes and writing down a few sentences. Other days, it was writing a whole two pages without a break. I think forcing myself to sit with my thoughts each day made a big difference compared to just writing when inspiration struck.\n\n\nIn my opinion, the most elusive part of preparing for my oral exam was anticipating questions and preparing for those. Writing the proposal was more straightforward and had an end product, but this preparation was a freer process. To organize my thoughts, I made a document containing important ideas, YouTube videos, and papers with my own comments. For basic statistics topics, such as UMAP and k-means clustering, I watched a lot of StatQuest videos on YouTube. Since my research is in genomics, specifically spatially resolved transcriptomics, I started out with review papers and read more specific papers if they were relevant. I also read a few papers about spatially resolved transcriptomics in neuroscience, since that is the application area that our collaborators work in. I re-watched recordings from a spatial statistics course that was offered in our department a couple years ago since my research involves modeling spatial data with nearest neighbor Gaussian processes.\nI spent about two months writing my proposal between mid-September to mid-November. In parallel with writing my proposal and up until my exam, I asked my advisor for one extra meeting per week to discuss questions that came up. These ranged from the public health impacts of our project, why the Poisson and negative binomial distributions are used to model RNA sequencing counts data, suggestions for figures, false discovery rates, technologies for spatially resolved transcriptomics data, and everything in between. I compiled questions each week that came up from writing my proposal and from my preparation process to discuss with her, and this process transformed my oral exam preparation.\nAfter sending out my proposal and slides to my committee, I asked each member to meet individually for about thirty minutes. I would highly recommend these meetings to ease nerves and to target specific areas for the final preparation process. I gave a five minute overview of my proposal to start each meeting, then they asked me questions and suggested some topics to focus on for their section of questions. I appreciated that all of my committee members were so encouraging and reassuring during these meetings.\nMy last preparation method was doing a practice run with a couple of my peers about 10 days before my exam. I took them through my 15 minute presentation and then answered their questions. I found this practice very reassuring since I could answer a lot of their questions, showing me that I was more prepared than I realized. I would recommend this to others preparing for their oral exams because it is harder to explain a concept orally compared to writing it down.\nUp until this point, my preparation had gone pretty smoothly. I definitely felt stressed out, but I was feeling ready to get it over with. Unfortunately, I ended up having COVID-19 the week before my exam! I had to spend most of my time resting and was not able to think about the exam at all. My exam ended up being over Zoom instead of in-person since I was still sick. In hindsight, I’m happy that I was a few days ahead of my preparation schedule and could take the time to rest.\nNevertheless, the oral exam went pretty smoothly. I started out with my presentation. Then, each member had 10-15 minutes to ask me a series of questions. These often related to the questions or topics we discussed in our individual meetings. I also had to answer a few basic biostatistics questions from my first year coursework. There were a few questions that I did not know or could not remember the answers to, so I just told them this and moved on to the next question. After the last committee member asked their questions, I left the room for about 10 minutes so they could deliberate. When I returned, they announced that I had passed! They also gave me a couple of suggestions for my research moving forward.\nPerhaps a trite phrase, but my main advice would be to believe in yourself. Your advisor would not let you go into this process without being ready for it and you have more knowledge than you realize.\n\n\n\n\n\n\nCoffee\n\n\n\nIf you found this blog post helpful and would like to support my work, feel free to buy me a coffee.\n\n\n\n\n\n\nReferences\n\n“My Preliminary Oral Exam.” 2020. https://blog.albertkuo.me/post/my-preliminary-oral-exam/."
  },
  {
    "objectID": "posts/2024-05-06-sim-part-1/index.html",
    "href": "posts/2024-05-06-sim-part-1/index.html",
    "title": "simulations part 1",
    "section": "",
    "text": "For one of my spatial transcriptomics research projects, I am working on simulating data to evaluate the sensitivity and specificity of our method. I wanted to document the process simulating counts data, as there are many learning opportunities along the way. This post is the first part, in which I will discuss understanding false discovery rate (FDR) in a basic differential expression analysis.\nThe overall framework of the simulation is as follows. First, I simulated gene expression data for 1,000 genes with 20 samples. Second, I manipulated the gene expression values for 100 genes in half the samples to be much higher. Third, I computed differential expression tests for each gene. Fourth, I explored the p-values and false discovery rates. After a lot of trial and error, this code is the final version of the simulation. I will explain each parameter and variable value choice in greater detail below.\n\n# simulate gene expression data for 1000 genes and 20 samples\n# repeat for 100 iterations\n\nset.seed(3)\nnum_iterations &lt;- 100\nalpha_levels &lt;- seq(0.001, 0.1, by = 0.001) \nnum_genes &lt;- 1000\nnum_de_genes &lt;- 100\nnum_samples &lt;- 20\nnum_half_samples &lt;- num_samples / 2\n\nfdr_percent_df &lt;- data.frame(matrix(0, nrow = num_iterations, ncol = length(alpha_levels)))\n\nfor (i in 1:num_iterations) {\n  # generate gene expression data\n  # use individual lambda for each gene\n  lambda &lt;- runif(num_genes, min = 5, max = 8)\n  gene_expr &lt;- matrix(0, nrow = num_genes, ncol = num_samples)\n  for (k in 1:num_genes) {\n    gene_expr[k, ] &lt;- rpois(num_samples, lambda =  lambda[i])\n  }\n\n  de_genes &lt;- sample(1:num_genes, num_de_genes)\n  \n  fold_change &lt;- runif(num_de_genes, min = 5, max = 8)\n  for (j in 1:num_de_genes) {\n    gene_expr[de_genes[j], 1:num_half_samples] &lt;- gene_expr[de_genes[j], 1:num_half_samples] * fold_change[j]\n  }\n  \n  gene_expr &lt;- data.frame(gene_expr)\n  # create indicator for DE genes\n  gene_expr$is_de &lt;- FALSE\n  gene_expr$is_de[de_genes] &lt;- TRUE\n  \n  p_values &lt;- apply(gene_expr, 1, function(x) {\n    t_test &lt;- t.test(x[1:num_half_samples], x[num_half_samples+1:num_samples])\n    t_test$p.value\n  })\n  \n  gene_expr$p_values &lt;- p_values\n  \n  adj_p_values &lt;- p.adjust(p_values,  method = \"fdr\")\n  gene_expr$adj_p_values &lt;- adj_p_values\n  \n  fdr_percent &lt;- sapply(alpha_levels, function(alpha) {\n    num_positive_tests &lt;- sum(adj_p_values &lt;= alpha)\n    # calculate the number of false positives as the number of non DE genes with adjusted p-value &lt;= alpha\n    num_false_positives &lt;- sum(gene_expr$adj_p_values[!gene_expr$is_de] &lt;= alpha)\n    if (num_positive_tests &gt; 0) {\n      fdr &lt;- num_false_positives / num_positive_tests * 100\n    } else {\n      fdr &lt;- 0\n    }\n    return(fdr)\n  })\n  fdr_percent_df[i, ] &lt;- fdr_percent\n}\n\nfdr_percent_avg &lt;- colMeans(fdr_percent_df)\nfdr_percent_sd &lt;- apply(fdr_percent_df, 2, sd)\n\n# plot unadjusted p-values\nhist(gene_expr$p_values, breaks = 100, xlab = \"p-value\", main = \"unadjusted p-values\",\n     sub = \"p-value histogram for final iteration\")\n\n\n\n# plot FDR vs significance\nplot(alpha_levels, fdr_percent_avg, type = \"l\",\n     xlab = \"significance level\", ylab = \"average FDR (%)\",\n     main = \"FDR vs. significance level\",\n     ylim = c(-1, 10))\nlines(alpha_levels, fdr_percent_avg + fdr_percent_sd, lty = 2)\nlines(alpha_levels, fdr_percent_avg - fdr_percent_sd, lty = 2)\n\n\n\n\nTo simulate the gene expression data, I used the Poisson distribution because it is a discrete probability distribution that can handle count data. In order to draw observations from the Poisson distribution, we need a parameter value, lambda, to represent the mean and specify the distribution. I drew 1,000 observations from the uniform distribution from 5 to 8 to get the lambda values. Initially, I had used the uniform distribution from 1 to 5. This distribution created many Poisson observation values that were equal to 0 within each gene, which remained 0 when multiplied by the fold-change factor. I wanted the differentially expressed gene counts in each sample to be increased when multiplied by the fold-change factor, not to remain 0.\nAt this point, I have initial gene expression data for 1,000 genes for 20 samples. In order to create differentially expressed genes, I randomly chose 100 genes to manipulate. I sampled 100 fold-change factors from the uniform distribution from 5 to 8, then multiplied the chosen gene counts for the 10 samples in condition A by the fold-change factors. I initially used the uniform distribution from 1 to 2 for the fold-change factors, but I increased the magnitudes of these factors to make sure that differential expression could be detected with very large effect sizes. For example, using the uniform distribution from 1 to 2 instead of 5 to 8 gives us the p-value distribution below. This post by David Robinson (“How to Interpret a p-Value Histogram” 2014) helped me understand how to interpret a p-value histogram. If we ignore the peak at 0, the null p-values should be uniformly distributed between 0 and 1 by definition. The peak at 0 is where most of the alternative hypotheses should lie.\n\n\n\n\n\nAs stated earlier, I simulated data for 1,000 genes and 20 samples. Out of the 1,000 genes, 100 are true differentially expressed genes. I chose 100 out of 1,000 genes to make this simulation realistic as there are typically thousands of genes in these types of data, but only a small percent are differentially expressed. There are 10 samples in condition A and 10 samples in condition B (these can be thought of as diseased vs. control). Reducing the number of samples per condition makes the p-value distribution less like what we would expect. For example, using 3 samples per condition instead of 10 gives us the p-value distribution below.\n\n\n\n\n\nAfter generating the simulated gene counts data and creating differentially expressed genes, the next step is computing the differential expression test. For each gene, I used a two sample t-test where the first sample is the gene counts values for condition A and the second sample is the gene counts values for condition B. These tests result in 1,000 total p-values, which I explored with a p-value histogram. The histogram is anti-conservative, which is what I was hoping for.\n\n\n\n\n\nAfter examining the p-value histogram, I used the Benjamini & Hochberg procedure to correct for multiple hypothesis testing. I used the adjusted p-values to analyze the FDR at various significance levels. For example, consider a significance level of 0.05. The total number of positive tests is the number of adjusted p-values that are less than 0.05. The number of false positives is the number of non-differentially expressed genes with adjusted p-values that are less than 0.05. The FDR is equal to the number of false positives divided by the total number of positive tests. So, we are rejecting as many null hypotheses as possible while guaranteeing that no more than 5% of those rejected null hypotheses are false positives. I repeated this FDR calculation for significance levels ranging from 0.001 to 0.1.\n\n\n\n\n\nFinally, I repeated the entire experiment 100 times. These repeated experiments allowed me to calculate confidence intervals for the FDR for each level of significance. I also calculated the true positive rate (TPR) and true negative rate (TNR), also known as sensitivity and specificity, at various levels of significance. Increasing the sample size and fold-change factors make it easier for the differential expression test to find DE genes, which also makes the specificity very high.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoffee\n\n\n\nIf you found this blog post helpful and would like to support my work, feel free to buy me a coffee.\n\n\n\n\n\n\nReferences\n\n“How to Interpret a p-Value Histogram.” 2014. http://varianceexplained.org/statistics/interpreting-pvalue-histogram/."
  }
]